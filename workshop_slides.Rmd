---
title: "Workshop: Building Neural Networks with Keras and TensorFlow in R"
author: "Dr. Shirin Glander"
date: "08./09.11.2018 - Munich"
fontsize: 12pt
output:
  beamer_presentation:
    #theme: "Boadilla"
    #colortheme: "rose"
    #fonttheme: "professionalfonts"
    #toc: true
    highlight: tango
    keep_tex: true
    fig_width: 7
    fig_height: 6
    fig_caption: false
    latex_engine: xelatex
    slide_level: 2
    includes:
      before_body: before_body.tex
      in_header: custom.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(xtable)
```

# About me

## How I ended up here

![](img/my_story_wml3zm.png)

# Keras

## What is Keras?

- high-level API
- library in Python that allows you to build neural networks
- works on top of TensorFlow, Theano, CNTK

### Why use Keras?

- enables easy and fast prototyping
- highly modular & flexible
- supports both convolutional networks and recurrent networks, as well as combinations
- highly extensible (layers)
- runs on CPU and GPU.

\vspace{0.5cm}
\centering
![](img/keras_logo.png){#id .class width=200 height=50px}

## About this workshop

You will learn

- the basics of deep learning
- what cross-entropy and loss is
- about activation functions
- how to optimize weights and biases with backpropagation and gradient descent
- how to build (deep) neural networks with Keras and TensorFlow
- how to save and load models and model weights
- how to visualize models with TensorBoard
- how to make predictions on test data

## Workshop material

### Scripts and code

```{}
git clone \
  https://github.com/ShirinG/workshop_keras_tensorflow_r.git
```

### Keras and TensorFlow

```{}
docker pull shiringlander/r-keras
docker run -d -p 8787:8787 \
  -v /path/to/keras_tensorflow_workshop_material:/home/rstudio/ \
  shiringlander/r-keras
```

### see also 00_setup.html

# Neural Networks

## Introduction to Neural Networks (NN)

- machine learning framework
- attempts to mimic the learning pattern of the brain (biological neural networks)
- Artificial Neural Networks = ANNs

\vspace{0.5cm}
\centering
![](img/neural_net_brain.jpeg){#id .class width=400 height=200}

## Perceptrons

- Input (multiplied by weight)
- Bias
- Activation function
- Output

\vspace{0.5cm}
\centering
![](img/perceptron.jpg){#id .class width=400 height=200}

## Activation functions

\centering
![](img/activation_functions.jpg){#id .class width=400 height=200}

## Activation functions

\centering
![](img/non_linearity.png){#id .class width=400 height=200}

## Multi-Layer Perceptrons (MLP)

- multiple layers of perceptrons
- input
- output
- hidden layers

\vspace{0.5cm}
\centering
![](img/mlp.jpg){#id .class width=300}

# Practical Part 1: Neural nets in R

## A general workflow

- Input data
- Data preprocessing
- Training, validation and test split
- Modeling
- Predictions and Evaluations

\vspace{0.5cm}
\centering
![](img/workflow.jpg){#id .class height=280}

## Why use R

- open-source, cross-platform
- established language (there are lots and lots of packages)
- high quality packages with proper documentation (CRAN)
- graphics capabilities - ggplot2!!!
- RStudio + R Markdown!!
- community support!

\vspace{0.5cm}
\centering
![](img/IMG_3724.jpg){#id .class width=200}

## Features & data preprocessing

```{r echo=TRUE}
library(ISLR)
```

```{r eval=FALSE, echo=TRUE}
print(head(College))
#summary(College)
```

```{r eval=FALSE, echo=FALSE}
print(xtable(head(College)))
```

\begin{table}[h!]
    \begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{rlrrrrrrrrrrrrrrrrr}
  \hline
 & Private & Apps & Accept & Enroll & Top10perc & Top25perc & F.Undergrad & P.Undergrad & Outstate & Room.Board & Books & Personal & PhD & Terminal & S.F.Ratio & perc.alumni & Expend & Grad.Rate \\ 
  \hline
Abilene Christian University & Yes & 1660.00 & 1232.00 & 721.00 & 23.00 & 52.00 & 2885.00 & 537.00 & 7440.00 & 3300.00 & 450.00 & 2200.00 & 70.00 & 78.00 & 18.10 & 12.00 & 7041.00 & 60.00 \\ 
  Adelphi University & Yes & 2186.00 & 1924.00 & 512.00 & 16.00 & 29.00 & 2683.00 & 1227.00 & 12280.00 & 6450.00 & 750.00 & 1500.00 & 29.00 & 30.00 & 12.20 & 16.00 & 10527.00 & 56.00 \\ 
  Adrian College & Yes & 1428.00 & 1097.00 & 336.00 & 22.00 & 50.00 & 1036.00 & 99.00 & 11250.00 & 3750.00 & 400.00 & 1165.00 & 53.00 & 66.00 & 12.90 & 30.00 & 8735.00 & 54.00 \\ 
  Agnes Scott College & Yes & 417.00 & 349.00 & 137.00 & 60.00 & 89.00 & 510.00 & 63.00 & 12960.00 & 5450.00 & 450.00 & 875.00 & 92.00 & 97.00 & 7.70 & 37.00 & 19016.00 & 59.00 \\ 
  Alaska Pacific University & Yes & 193.00 & 146.00 & 55.00 & 16.00 & 44.00 & 249.00 & 869.00 & 7560.00 & 4120.00 & 800.00 & 1500.00 & 76.00 & 72.00 & 11.90 & 2.00 & 10922.00 & 15.00 \\ 
  Albertson College & Yes & 587.00 & 479.00 & 158.00 & 38.00 & 62.00 & 678.00 & 41.00 & 13500.00 & 3335.00 & 500.00 & 675.00 & 67.00 & 73.00 & 9.40 & 11.00 & 9727.00 & 55.00 \\ 
   \hline
\end{tabular}
}
    \end{center}
\end{table}

```{r echo=TRUE}
# Create vector of column max and min values
maxs <- apply(College[, 2:18], 2, max)
mins <- apply(College[, 2:18], 2, min)

# Use scale() and convert the resulting matrix to a data frame
scaled.data <- scale(College[, 2:18],
                     center = mins, 
                     scale = maxs - mins) %>%
  as.data.frame()

summary(scaled.data)
```

## Training, validation & testing

- balance between generalization and specificity
- to prevent over-fitting!
- validation sets or cross-validation

\vspace{0.5cm}
\centering
![](img/overfitting_explained.jpg){#id .class height=200}

## Training, validation & testing

\centering
![](img/validation.jpg){#id .class height=280}

## Train and test split

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Convert column Private from Yes/No to 1/0
Private <- as.numeric(College$Private)-1
data <- cbind(Private, scaled.data)

summary(as.factor(data$Private))

library(caret)
set.seed(42)

# Create split
idx <- createDataPartition(data$Private, p = .75, list = FALSE)
train <- data[idx, ]
test  <- data[-idx, ]
```

## Training with caret

```{r echo=TRUE}
?train
```

> This function sets up a grid of tuning parameters for a number of classification and regression routines, fits each model and calculates a resampling based performance measure.

## Modeling

```{r echo=TRUE}
# 3 x 5 repeated CV
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 3,
                           savePredictions = TRUE)
```

```{r echo=TRUE, warning=FALSE, message=FALSE}
set.seed(42)
nn <- train(factor(Private) ~ ., 
            data = train, 
            method = "nnet", 
            preProcess = c("scale", "center"),
            trControl = fitControl,
            verbose= FALSE)
```

## Plotting

```{r}
plot(nn)
```

```{r eval=FALSE, echo=FALSE}
library(neuralnet)
n <- names(train)
f <- as.formula(paste("Private ~", paste(n[!n %in% "Private"], collapse = " + ")))
nn2 <- neuralnet(f,data=train,hidden=c(5,3),linear.output=T)
plot(nn2)
```

## Predictions and Evaluations

```{r echo=TRUE}
#predicted.nn.values <- predict(nn, test[, 2:18], type = "prob")
predicted.nn.values <- predict(nn, test[, 2:18], type = "raw")

cbind(predicted.nn.values, test) %>%
  count(predicted.nn.values, Private)

# print results
print(head(predicted.nn.values, 3))
```

## Confusion Matrix

```{r echo=TRUE}
caret::confusionMatrix(predicted.nn.values, as.factor(test$Private))
```

## ...continue on your own...

- get acquainted with R
- try out different hidden layer combinations
- set other hyperparameters in the `train()` function (see `?train`)
- try different ratios of training and test data
- explore different normalization techniques, e.g. dividing by the maximum value of each column
- ...

# How do Neural Nets learn?

## Classfication with supervised learning

\centering
![](img/supervised.jpg){#id .class height=280}

## How do Neural Nets learn?

\centering
![](img/nn_learn.jpg){#id .class height=280}

## Softmax

\centering
![](img/softmax.jpg){#id .class height=280}

## Cross-entropy

\centering
![](img/cross-entropy.jpg){#id .class height=280}

## Backpropagation

\vspace{0.5cm}
\centering
![](img/backpropagation.jpg){#id .class width=400}

## Gradient Descent Optimization

\vspace{0.5cm}
\centering
![](img/gradient_descent.jpg){#id .class width=400}


# Deep Learning with Keras & TensorFlow

## Deep Neural Networks

- loosely defined as a NN with more than 2 hidden layers
- use for COMPLEX problems!

\vspace{0.5cm}
\centering
![](img/deep_learning.jpg){#id .class width=400}

## Deep Neural Networks

\centering
![](img/dl_tasks.png){#id .class width=400}

## Keras APIs

\centering
![](img/keras_apis.png){#id .class width=400}

## Keras Basics

You are dealing with layers:

- input layer ... output layer
- dropout layer
- noise layer
- pooling layer
- normalization layer
- embedding layer

- activation function for each layer
- regularization techniques
- optimization functions

- you can save models, layers, weights

## Keras layers

\centering
![](img/keras_layers.png){#id .class width=400}

## Keras possibilities

\centering
![](img/keras_possibilities.png){#id .class width=400}

# TensorFlow

## TensorFlow

- originally developed by Google Brain
- open-source software for deep learning
- based on flow charts / graphs

\vspace{0.5cm}
\centering
![](img/tensorflowlogo.png){#id .class width=300 height=200px}

## Tensors

- tensors ~ multidimensional arrays

- images can be converted to arrays
- 2D for black-and-white, 3x 2D for RGB

- can be processed in PARALLEL

\vspace{0.5cm}
\centering
![](img/tensor_array.png){#id .class width=300 height=200px}

## Tensors

\centering
![](img/tensors.jpg){#id .class width=300 height=300px}

## Graphs

- computational graphs to represent operations
- each node takes zero or more tensors as inputs and ...
- ... performs a mathematical operation ...
- ... to produce a tensor as an output
- visualization with TensorBoard

\vspace{0.5cm}
\centering
![](img/graph.jpeg){#id .class width=400 height=300px}

## TensorBoard

- suite of visualization tools
- visualize TensorFlow graph, 
- plot quantitative metrics about the execution of your graph,
- etc.

\vspace{0.5cm}
\centering
![](img/tensorboard_example.png){#id .class width=400 height=400px}

# Practical Part 2: Sequential models with Keras

## Sequential model

- linear stack of layers
- input layer -> hidden layer -> output layer

### The MNIST example

- MNIST handwritten digits dataset
- labels: numbers from 0 - 9
- images: 28x28 pixels

\vspace{0.5cm}
\centering
![](img/mnist.png){#id height=130px}

## ...continue on your own...

- try out different hidden layer combinations
- change & set other hyperparameters in the layers, e.g. learning rate
- change optimizer
- change activation function
- change epochs
- explore different normalization techniques, e.g. `scale()`
- ...

# Practical Part 2: Convolutional Neural Networks

## How does a computer learn to see?

\centering
![](img/cnn_intro.jpg){#id .class width=400}

## Image input data

\centering
![](img/image_input.jpg){#id .class width=400}

## Convolutional Neural Networks (CNNs)

\centering
![](img/cnn_architecture.jpg){#id .class width=400}

## CNN layers

\centering
![](img/cnn_layers.jpg){#id .class width=400}

## Convolution & Pooling

### Convolution

The convolution process calculates an **activation map** for each region that will be triggered if the learned network recognizes a typical feature used for classification.

### Pooling
Pooling layers combine the stacked outputs (i.e. the activation maps) of all subsets of neurons from preceding convolutional layers.

## CNN architecture

- most commonly used in image recognition
- not fully connected
- convolutional layers + pooling layers

\vspace{0.5cm}
\centering
![](img/cnn.jpg){#id .class width=400}

## MLP vs CNN

\centering
![](img/mlp_cnn.jpg){#id .class width=400}

## History of CNNs

\centering
![](img/history_nn.jpg){#id .class width=400}

## CNN architectures

\centering
![](img/cnn_architectures.jpg){#id .class width=400}

## ...continue on your own...

Build a CNN with the MNIST dataset

- try out different combinations of layers
- try out different hyperparameters
- ...

## The CIFAR example

- image classification dataset
- labels: 10 classes
- images: 32x32 color (RGB)

\vspace{0.5cm}
\centering
![](img/cifar.png){#id height=180px}

## ...continue on your own...

- try out different combinations of layers
- try out different hyperparameters
- have a look at other types of DNNs on the Keras page
- try out different datasets, e.g. from Kaggle or UC Irvine's ML repository
- ...

## Additional resources

- [Classify a fruit dataset from Kaggle...](https://shirinsplayground.netlify.com/2018/06/keras_fruits/)
- [... and explain it with LIME](https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/)
- [Use pre-trained models, either directly or modify them](https://keras.rstudio.com/articles/applications.html)
- [Free notebooks from "Deep Learning with R"](https://github.com/jjallaire/deep-learning-with-r-notebooks)

# Thank you!

## Thank you

![](img/stay_connected.png){#id height=180px}

